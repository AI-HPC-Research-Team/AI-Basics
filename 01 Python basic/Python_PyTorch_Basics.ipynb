{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "574821c8",
      "metadata": {
        "id": "574821c8"
      },
      "source": [
        "# Python & PyTorch Basics\n",
        "\n",
        "**Recommended runtime:** Google Colab (GPU optional) or local JupyterLab.\n",
        "\n",
        "- ✅ Python basics: syntax, containers, functions, files, iteration & higher-order functions\n",
        "- ✅ PyTorch basics: Tensor, GPU device, Autograd, `nn.Module`, `DataLoader`, train/eval loops\n",
        "- ✅ Example 1 (NLP): Bag-of-Words small example (from scratch)\n",
        "- ✅ Example 2 (CV): FashionMNIST quick training (MLP; runs on CPU/GPU)\n",
        "- ⛳ Optional: Tiny BERT fine-tuning (1 epoch) with `transformers`\n",
        "\n",
        "> Notes: cells try to **gracefully degrade**—if a tool is missing, the notebook prints alternatives or skips steps.  \n",
        "> Last updated: 2025-11-12 10:12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a557dcb",
      "metadata": {
        "id": "9a557dcb"
      },
      "source": [
        "## 0) Runtime Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b137fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64b137fd",
        "outputId": "8338758a-c87e-4561-c6d2-f5ef987b1eb6"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, sys, platform\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Platform:\", platform.platform())\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ\n",
        "print(\"In Colab:\", IN_COLAB)\n",
        "try:\n",
        "    import torch\n",
        "    print(\"PyTorch:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print(\"PyTorch not found:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fmXF63qkWOKy",
      "metadata": {
        "id": "fmXF63qkWOKy"
      },
      "source": [
        "## 1) **Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3XkvLfwkWJx4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XkvLfwkWJx4",
        "outputId": "64b6a849-57fe-4d14-9284-4f3746d9259a"
      },
      "outputs": [],
      "source": [
        "%pip install -qq -U transformers datasets accelerate pyarrow==19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4JzmkT-vicrF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JzmkT-vicrF",
        "outputId": "450ffb7f-d141-4756-e841-2bc0d9b68722"
      },
      "outputs": [],
      "source": [
        "import os, wandb\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = getpass(\"Enter your WANDB_API_KEY: \")\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a3a7f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86a3a7f9",
        "outputId": "143c7505-2329-41b2-c496-ac94476d24d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"Pandas:\", pd.__version__)\n",
        "print(\"Matplotlib:\", plt.matplotlib.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ee622c",
      "metadata": {
        "id": "09ee622c"
      },
      "source": [
        "## 2) Python Basics · Syntax / Containers / Functions / Files / Iteration\n",
        "Covers numbers/strings, lists/tuples/dicts, functions/args, loops, file I/O, list comprehensions, and basic higher-order functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52839ca9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52839ca9",
        "outputId": "d88fc7e0-d574-4646-8f2d-816c5956069c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Numbers, strings, lists/tuples/dicts\n",
        "x = 5\n",
        "y = 2.5\n",
        "s = \"Hello, Python!\"\n",
        "lst = [1, 2, 3]\n",
        "tup = ('a', 1)\n",
        "d = {'k': 3, 'v': 9}\n",
        "print(x+y, s.upper(), lst, tup, d['k'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778f25dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "778f25dc",
        "outputId": "f92652ae-ee65-426b-9996-ae4aa7931c31"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Functions & keyword-only args\n",
        "def greet(name, title=\"Dr.\", *, excited=False):\n",
        "    msg = f\"Hi, {title} {name}\"\n",
        "    return msg + \"!!!\" if excited else msg\n",
        "\n",
        "print(greet(\"Smith\"))\n",
        "print(greet(\"Ada\", title=\"Prof.\", excited=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e1dadff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e1dadff",
        "outputId": "2e1d6982-0460-4e8e-da17-1dc642c12904"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loops, comprehensions, higher-order utilities\n",
        "from functools import lru_cache, reduce\n",
        "squares = [i*i for i in range(6)]\n",
        "even_squares = [z for z in squares if z % 2 == 0]\n",
        "sum_squares = reduce(lambda a,b: a+b, squares, 0)\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def fib(n):\n",
        "    return n if n < 2 else fib(n-1)+fib(n-2)\n",
        "\n",
        "print(\"squares:\", squares)\n",
        "print(\"even_squares:\", even_squares)\n",
        "print(\"sum_squares:\", sum_squares)\n",
        "print(\"fib(20):\", fib(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57342657",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57342657",
        "outputId": "c212f386-0e0e-49e4-bf78-f15966f086a8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Simple file I/O\n",
        "from pathlib import Path\n",
        "p = Path(\"demo.txt\")\n",
        "p.write_text(\"First line\\nSecond line\\nThird line\\n\", encoding=\"utf-8\")\n",
        "print(\"File content:\")\n",
        "print(p.read_text(encoding=\"utf-8\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50ec7c4",
      "metadata": {
        "id": "a50ec7c4"
      },
      "source": [
        "## 3) PyTorch Basics · Tensor / GPU / Autograd / nn\n",
        "Goal: get a quick grasp of tensors, device moves, broadcasting, autograd, and building a minimal `nn.Module`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e94015b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e94015b",
        "outputId": "34ba68e4-c321-40e0-90c8-04fb51b17103"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "a = torch.randn(2,3, device=device)\n",
        "b = torch.randn(3,2, device=device)\n",
        "c = a @ b    # matmul\n",
        "print(\"a:\", a.shape, \"b:\", b.shape, \"c:\", c.shape)\n",
        "x = torch.arange(6, dtype=torch.float32, device=device).reshape(2,3)\n",
        "v = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
        "print(\"Broadcast:\", (x+v).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa343b8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa343b8c",
        "outputId": "df3c9eaa-1423-4438-d1e0-a3c3ab6a2e18"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Autograd demo: y = (x^2).sum() -> dy/dx = 2x\n",
        "x = torch.randn(4, requires_grad=True)\n",
        "y = (x**2).sum()\n",
        "y.backward()\n",
        "print(\"x:\", x)\n",
        "print(\"x.grad:\", x.grad)\n",
        "# Detach to stop tracking\n",
        "z = x.detach()\n",
        "print(\"Detached requires_grad:\", z.requires_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45fb13a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45fb13a6",
        "outputId": "7ff1aa7a-5a57-4e4f-80c2-59378012af92"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Minimal nn.Module\n",
        "import torch.nn as nn\n",
        "class TinyNet(nn.Module):\n",
        "    def __init__(self, d_in=10, d_h=16, d_out=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_in, d_h),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_h, d_out)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = TinyNet().to(device)\n",
        "print(model)\n",
        "dummy = torch.randn(5, 10, device=device)\n",
        "logits = model(dummy)\n",
        "print(\"logits:\", logits.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63adf89c",
      "metadata": {
        "id": "63adf89c"
      },
      "source": [
        "## 4) Example #1 · Bag-of-Words (from scratch, tiny dataset)\n",
        "- Tokenization → vocab → vectorization → linear classifier (`nn.Linear`).\n",
        "- Good for showing the full pipeline: preprocessing → tensors → training → evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a10c06f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a10c06f8",
        "outputId": "f472fd71-105e-4de6-980b-426ee01c4a83"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re, random, torch, torch.nn as nn, torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data = [\n",
        "    (\"deep learning changes everything\", 1),\n",
        "    (\"neural networks are powerful\", 1),\n",
        "    (\"this movie was fantastic\", 1),\n",
        "    (\"awful plot and bad acting\", 0),\n",
        "    (\"terrible movie and boring\", 0),\n",
        "    (\"an excellent and enjoyable film\", 1),\n",
        "    (\"bad direction and poor script\", 0),\n",
        "    (\"i loved the visuals\", 1),\n",
        "]\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "def tokenize(s): return re.findall(r\"[a-z]+\", s.lower())\n",
        "vocab = {}\n",
        "for s,_ in data:\n",
        "    for tok in tokenize(s):\n",
        "        if tok not in vocab: vocab[tok] = len(vocab)\n",
        "V = len(vocab)\n",
        "\n",
        "def vectorize(s):\n",
        "    x = torch.zeros(V)\n",
        "    for tok in tokenize(s):\n",
        "        if tok in vocab:\n",
        "            x[vocab[tok]] += 1.0\n",
        "    return x\n",
        "\n",
        "X = torch.stack([vectorize(s) for s,_ in data])\n",
        "y = torch.tensor([lbl for _,lbl in data], dtype=torch.long)\n",
        "n_train = int(0.75*len(data))\n",
        "Xtr, Xte = X[:n_train], X[n_train:]\n",
        "ytr, yte = y[:n_train], y[n_train:]\n",
        "\n",
        "model = nn.Sequential(nn.Linear(V, 2)).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = optim.Adam(model.parameters(), lr=0.05)\n",
        "\n",
        "Xtr_d = Xtr.to(device)\n",
        "ytr_d = ytr.to(device)\n",
        "for epoch in range(80):\n",
        "    opt.zero_grad()\n",
        "    logits = model(Xtr_d)\n",
        "    loss = loss_fn(logits, ytr_d)\n",
        "    loss.backward(); opt.step()\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"epoch {epoch+1:03d} loss={loss.item():.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(X, y):\n",
        "    logits = model(X.to(device))\n",
        "    pred = logits.argmax(dim=1).cpu()\n",
        "    acc = (pred == y).float().mean().item()\n",
        "    return acc, pred\n",
        "\n",
        "acc, pred = evaluate(Xte, yte)\n",
        "print(\"Test acc:\", round(acc, 3))\n",
        "print(\"Pred vs true:\", list(zip(pred.tolist(), yte.tolist())))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bfc0c44",
      "metadata": {
        "id": "1bfc0c44"
      },
      "source": [
        "## 5) Example #2 · FashionMNIST (MLP quick training)\n",
        "- Pipeline: `Dataset/DataLoader → Model → Loss/Optimizer → Train/Eval`\n",
        "- Runs on CPU by default; uses GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6624c117",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6624c117",
        "outputId": "ccf91093-04d0-47d4-8be3-116349f8d386"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim, os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tfm = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=tfm)\n",
        "test_ds  = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=tfm)\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "model = MLP().to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total, correct, total_loss = 0, 0, 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "        loss.backward(); opt.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        correct += (logits.argmax(1) == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, correct, total_loss = 0, 0, 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        correct += (logits.argmax(1) == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "for epoch in range(3):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader)\n",
        "    te_loss, te_acc = evaluate(model, test_loader)\n",
        "    print(f\"epoch {epoch+1} | train acc {tr_acc:.3f} | val acc {te_acc:.3f}\")\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"checkpoints/fmnist_mlp.pt\")\n",
        "print(\"Saved to checkpoints/fmnist_mlp.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92afd92f",
      "metadata": {
        "id": "92afd92f"
      },
      "source": [
        "## 6) (Optional) Tiny BERT Fine-tuning\n",
        "> Requires internet to install `transformers` / `datasets`. Runtime depends on your environment.\n",
        "This cell fine-tunes **`distilbert-base-uncased`** for 1 epoch on a small sample, demonstrating the Trainer API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc4bb66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953,
          "referenced_widgets": [
            "b041045f23754b07b33fb29791502937",
            "9bac6481986040b5b5fd54f938259697",
            "921047058b6a4571aa43a128a75fd4b6",
            "f1ebfdc3070a40449055aca35fbbfdc6",
            "0ad3d849a449479fb0eb4e0d37150163",
            "6f1645482fe5499e992715ff873d0edd",
            "23ab780fcf624d58a1283860ff4f905a",
            "02b4f1a54e574c04a8bf7c34310f266b",
            "4fea07bc01d444019450930df8e13652",
            "3199f891d69f41d99cdf626c689f3561",
            "61930f069af744bbbd55040dd882431b",
            "61369e8e650a4152bf198ef5cf1a700e",
            "eac0dfe44f9a4554af4fd36f28ed6218",
            "055a804834194d12be66e38d0b398079",
            "7413bf50d2494fb3a67480039da9ff21",
            "af8f863e2300404ea28adcdd3cadc311",
            "dd3fb8d453ad4f93b0711168911d4862",
            "2e511b9366d9491f909049300be48679",
            "ae457acaaac84dc0aa586eed48636382",
            "70e2dccccfca46b9b19577ebdb9cc1e1",
            "3ae997e1debf420c833ba03d387e1383",
            "cd946b1ba7c94e9fbdb5b71f1236fc50",
            "ac5b813094914611bf7f60397c9b4abb",
            "e3a2316bb3b6479c9a6c93f2b2b842f1",
            "89a3e9ab562b402bafa44cfb185d2fc1",
            "edb33a5a40a849bfbd6f140049dededa",
            "b6e609235f9445ce816da5b29306b432",
            "d287b772d3494d10b42f0a6c378ef97b",
            "a21efa5a2cfb408381dd9d4f23999013",
            "e91302060eb841df8a73fdfe2e189469",
            "f2e16c24c0c948a6ae3bd6b1f7a20f94",
            "0a36e8b6c4ea4036a66c411a89738dbd",
            "57bac5dfa18c486cbadb41fc87e3562a",
            "fcf4f4227ca0450191cc550cd5b8b25e",
            "c2eeade8860e40968308afd1930f15f6",
            "e73b2aa611814c16a43216a0e17e9423",
            "db76aef3029646b2889d76c35e1307cf",
            "0c001582ce2148d9984c22914e886603",
            "29b7f9a2e7aa407e8dc768ba302bbdeb",
            "355363caf7ce45f199e8667ac85686ad",
            "22f6d579d8654893a697f77a56d2d877",
            "fa6b46968955484bb3de54dfef3a8c89",
            "af6065fa4a53401a89335f57a7bd0c38",
            "45763ae77ff04c8bbfdb5a685cc4513c",
            "3ebff519ac454f9b8661d01e7c716c56",
            "7f5d980c4c8140119c82900986033042",
            "7a752b725b79447bb2e9456e49d192cd",
            "ac6413156fc14cdabce7bade3086457f",
            "2cdb839bbc7c42f4a999cf8f699855d5",
            "9d1b681a6bc54b41a85a7114ad70704d",
            "47ecdbe3ffcb40aa8d5f22e4421a71ed",
            "ac32df8e26b64a39a34197a9ab1b7f6b",
            "4ffca6be1089447a9bdd982a06b917bd",
            "520434158c994cb3b77cda91aabbb1d8",
            "29dff6c8147b44409efb1dea7f7e0b0a",
            "90e5dfc914ca44aaac86aae9430567bf",
            "3ec41541d3f64e37b3ebe0f1534659a2",
            "2ac2b4517d9442c9b7053cca5387f4c7",
            "cce9be30ca03455b93779eeda5578efe",
            "184fc62d9dae4e5eb82afb5b4cb51be7",
            "b1a19049ce8f473d8768dec4051cd1fd",
            "0ab8fee791554137a20e76e497bc2433",
            "f1a44e9d995e4fb5be1f3f4a3634ba06",
            "d0e105c8328f415186a30a9ac82e9616",
            "6ddf1e6ffb1e4b6e869807242d6d8df8",
            "53f9e80459fd49cd8f15e20237ce8e55",
            "8d06b17d238245ea8f54382a8e7ea3e6",
            "7c8e05913be84cd9bb11514cf2c66035",
            "241075715c354b14b892d5dcaaa78be4",
            "5ae650c383f54db585407dd284d4c4ee",
            "36ae1ec0dddc417e964cf2279f4e2dad",
            "382b03e7bd8941ea9380384d8e888db5",
            "2d985aa492a8409cbace504cf8ef1d4e",
            "ac72366203b04742bdf8fcee3306a15d",
            "b0930d97657c4aaf885cd16029785420",
            "7d2165f70bb24ba8a0fb0a75e9baceb0",
            "995a1af90fd84c748a6b71c507fcf80f",
            "1d74e125d22648cb87b73a8b594df9f8",
            "0d76c82b0f284370b38170d22532e861",
            "647ee1157a8b40438dd751f9811f4013",
            "cbb21cd6ca4f463fb4ae5e07778ca7ed",
            "8bdef6f29436453789d662595c9656ca",
            "d1f4efeffac24daea31e8bfc8dfd6f52",
            "61dd0b224a9747498431d4b91d8439ec",
            "9ec27403ac6e42889d332288a59b7b2b",
            "b62178ab70d849f591836ae73481c62e",
            "b7758352f96544e6a8b880cc0aab1ae2",
            "061ad0002782405eaadfd86830fe3aea",
            "4fbd27866f034c3aa228c4ba57bf22e7",
            "a65a914e41bd46dab75508cdc67171d2",
            "d59996bd5fa843f8832ad6e642066657",
            "1ac6b5858b3a4749a878fce42fb62cf6",
            "d658790af8d141279190b51dc2bea197",
            "0ce6eff0728c4723b99836f984e376f0",
            "ac860235a0ab43f88f5e2e02fcdd7f0e",
            "21338b4cc3594ddbb7601b943df441ef",
            "2f2bd742a10a482cb20b0dc87b1facc9",
            "bb545522a93a44e48366a3b7b9209f26",
            "90186f70925d4529b3fc7724c6520065",
            "27134d7f992049dd9131924a64964c4e",
            "03c4db1680574517b6cf557e6746cf44",
            "4b538fb118134ad68b62c09a65f72850",
            "8ffc0cf483504db383a8fa00bf4460c6",
            "16252c16bcea403aae35780176fe1d2e",
            "eab8fb6ea00940babac354a8ee17634d",
            "8df9b036a74a443c86b2bf185558de87",
            "94d3858b7bfd443c8d8e808d3f544f44",
            "631328969b8f458dbb394359c9462a5f",
            "d5b845911d9d4495b32adfee0baf4b21",
            "8d72b044441b45a289daf853ae23ca93",
            "8df44126f5814444bc57502d01a8e9d1",
            "20f833619495414fb47b5337ce1b0f49",
            "6a60ff262c1a498cb6677cfe18217f9c",
            "1313e2ee5eaf442ab42932c90d22fa03",
            "76897abd1bcc4fbba537795e47e03c29",
            "01a3e266720d44cd9606472d9ffb1761",
            "2398be9c104f4fc494ced6b72933ea53",
            "919f0fea31f54f95b89c6a57ac9c3686",
            "f342807b5c17411c95da3f45a02dec8b",
            "1c2cf2736b9946afaf2fb24390d34b77",
            "a773aa031b4c44a9aee6650748c3f3e0",
            "13342d3d5c604c1eaa0f38c495c53b45",
            "92c240285a6344deadec484fb272f1d6",
            "c0c3a085541a414aadc77c7c3cc441c6",
            "767a52e0d8444895b6da90d1287d7c59",
            "a115eaf8287a462a84803f551ee4cbe9",
            "e74e21beb1fd40508c200f54ec482062",
            "18489a9bb8aa439ca425131ab475cd56",
            "0dbf332020dc4b108265d49d78690e79",
            "8ff63c431c9243fbb614f19743b6bcf3",
            "1f4a6bdc45544ce5be629d65e056785d",
            "047015eb6bdb4c95bd05a4275911c3cf"
          ]
        },
        "id": "bbc4bb66",
        "outputId": "bfffd2cf-b3d3-43e5-c27f-405600e2526c"
      },
      "outputs": [],
      "source": [
        "\n",
        "RUN_BERT = True  # Set to True to actually run the cell\n",
        "if RUN_BERT:\n",
        "    from datasets import load_dataset\n",
        "    from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "    ds = load_dataset(\"ag_news\")\n",
        "    small_train = ds[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "    small_test  = ds[\"test\"].shuffle(seed=42).select(range(500))\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def tokenize(batch): return tok(batch[\"text\"], truncation=True)\n",
        "    small_train = small_train.map(tokenize, batched=True)\n",
        "    small_test  = small_test.map(tokenize, batched=True)\n",
        "\n",
        "    collate = DataCollatorWithPadding(tokenizer=tok)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"bert_demo\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        learning_rate=2e-5,\n",
        "        logging_steps=50,\n",
        "        fp16=False\n",
        "    )\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        import numpy as np\n",
        "        logits, labels = eval_pred\n",
        "        preds = logits.argmax(axis=-1)\n",
        "        acc = (preds == labels).astype(float).mean().item()\n",
        "        return {\"accuracy\": acc}\n",
        "\n",
        "    trainer = Trainer(model=model, args=args,\n",
        "                      train_dataset=small_train, eval_dataset=small_test,\n",
        "                      tokenizer=tok, data_collator=collate,\n",
        "                      compute_metrics=compute_metrics)\n",
        "    trainer.train()\n",
        "    print(trainer.evaluate())\n",
        "else:\n",
        "    print(\"Set RUN_BERT=True to run the tiny BERT fine-tuning demo.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4058bcd6",
      "metadata": {
        "id": "4058bcd6"
      },
      "source": [
        "---\n",
        "\n",
        "### Appendix · Quick Cheatsheet\n",
        "- **conda env**  \n",
        "`conda create -n my_env python=3.10` · `conda activate my_env` · `conda install -c conda-forge numpy`  \n",
        "`conda env export > env.yml` · `conda env create -f env.yml`\n",
        "- **mamba**: a faster drop-in replacement for conda\n",
        "- **pip**: `pip install packagename` (prefer conda first when mixing)\n",
        "- **PyTorch install**: see https://pytorch.org/get-started/ for CUDA-matched commands\n",
        "- **Training loop quartet**: `Dataset/DataLoader` → `Model(nn.Module)` → `Loss` → `Optimizer.step()`\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
